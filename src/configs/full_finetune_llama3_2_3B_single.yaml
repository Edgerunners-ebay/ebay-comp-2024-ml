# Config for single device QLoRA with lora_finetune_single_device.py
# using a Llama3.1 8B Instruct model
#
# This config assumes that you've run the following command before launching
# this run:
#   tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir /tmp/Meta-Llama-3.1-8B-Instruct --ignore-patterns "original/consolidated.00.pth"
#
# To launch on a single device, run the following command from root:
#   tune run lora_finetune_single_device --config llama3_1/8B_qlora_single_device
#
# You can add specific overrides through the command line. For example
# to override the checkpointer directory while launching training
# you can run:
#   tune run lora_finetune_single_device --config llama3_1/8B_qlora_single_device checkpointer.checkpoint_dir=<YOUR_CHECKPOINT_DIR>
#
# This config works only for training on single device.

#modified changes
# Originally from torchtune official docs https://github.com/pytorch/torchtune/blob/68d4f3ee1287ef60f3f17222ed7382fe5c33be96/recipes/configs/llama3_1/8B_qlora_single_device.yaml
# Added changes for activation offloading from https://github.com/pytorch/torchtune/blob/7045e965d84d518186b041f8e75dd389a3ef5f0e/recipes/configs/llama3_1/8B_qlora_single_device.yaml
# Modified with some local parameters to pass

# Model Arguments
model_dir: /home/017534556/ckpts/nov2-llama3.2-3B-instruct/
model:
    _component_: torchtune.models.llama3_2.llama3_2_3b

# Tokenizer
tokenizer:
    _component_: torchtune.models.llama3.llama3_tokenizer
    path: ${model_dir}/original/tokenizer.model
    max_seq_len: null

checkpointer:
    _component_: torchtune.training.FullModelHFCheckpointer
    checkpoint_dir: ${model_dir}
    checkpoint_files:
        [model-00001-of-00002.safetensors, model-00002-of-00002.safetensors]
    recipe_checkpoint: null
    output_dir: ${model_dir}
    model_type: LLAMA3
resume_from_checkpoint: False

# Dataset and Sampler
dataset:
    _component_: torchtune.datasets.alpaca_dataset
    EBAY_ROOT: "/Users/saish/Documents/data/ebay-comp-2024"
    OPTIMISED_FILES: "/Users/saish/Documents/data/ebay-comp-2024/optimised_files"
    EBAY_ROOT_HPC: "/home/017534556/data/ebay"
    OPTIMISED_FILES_HPC: "/home/017534556/data/ebay/optimised_files"
    tokenizer_path: ${model_dir}/original/tokenizer.model
    max_seq_len_custom: 16000

seed: null
shuffle: True
batch_size: 1
epochs: 3

# Optimizer and Scheduler
optimizer:
    _component_: torch.optim.AdamW
    lr: 2e-5
    fused: True
loss:
    _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
max_steps_per_epoch: null
gradient_accumulation_steps: 1

# Training env
device: cuda

# Memory management
enable_activation_checkpointing: True
custom_sharded_layers: ["tok_embeddings", "output"]
compile: False # set it to True for better memory and performance
optimizer_in_bwd: True

# Reduced precision
dtype: bf16

# Logging
output_dir: ${model_dir}/full_finetune_output/
metric_logger:
    _component_: torchtune.training.metric_logging.WandBLogger
    project: ebay-comp-2024
log_every_n_steps: 1
log_peak_memory_stats: True
