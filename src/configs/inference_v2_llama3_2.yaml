# Config for running the InferenceRecipe in generate.py to generate output from an LLM
#
# To launch, run the following command from root torchtune directory:
#    tune run generate --config generation
# Added my modifications to the original config located at https://github.com/pytorch/torchtune/blob/2c18c5bd92c6a29227e7a91ba1c56be4833eeb18/recipes/configs/generation.yaml

# Model arguments
model_dir: /home/017534556/ckpts/nov2-llama3.2-3B-instruct/
model:
    _component_: torchtune.models.llama3_2.llama3_2_3b

dataset:
    _component_: torchtune.datasets.alpaca_cleaned_dataset
    EBAY_ROOT: "/Users/saish/Documents/data/ebay-comp-2024"
    OPTIMISED_FILES: "/Users/saish/Documents/data/ebay-comp-2024/optimised_files"
    EBAY_ROOT_HPC: "/home/017534556/data/ebay"
    OPTIMISED_FILES_HPC: "/home/017534556/data/ebay/optimised_files"
    tokenizer_path: ${model_dir}/original/tokenizer.model
    max_seq_len_custom: 16000
    is_inference: True

checkpointer:
    _component_: torchtune.training.FullModelHFCheckpointer
    checkpoint_dir: ${model_dir}
    checkpoint_files: [hf_model_0001_2.pt, hf_model_0002_2.pt]
    output_dir: ${model_dir}
    model_type: LLAMA3

device: cuda
dtype: bf16

seed: 1234
log_level: INFO

# Tokenizer arguments
tokenizer:
    _component_: torchtune.models.llama3.llama3_tokenizer
    path: ${model_dir}/original/tokenizer.model
    max_seq_len: null

# Generation arguments; defaults taken from gpt-fast
max_new_tokens: 8000
temperature: 0.6 # 0.8 and 0.6 are popular values to try
top_k: 300

enable_kv_cache: True

quantizer: null

db_root: /home/017534556/projects/ebay-comp-2024/db
submission_table_name: nov3_full_finetune_llama3_2_3B_quiz_8000_max_new_tokens
